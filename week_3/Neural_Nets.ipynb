{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìò Applied Machine Learning - Week 3\n",
    "\n",
    "**Neural Networks and Convolutional Neural Networks**\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Before You Start\n",
    "\n",
    "### ‚öôÔ∏è Setup Requirements\n",
    "\n",
    "1. **Copy Week 2 code** - Create a file `blocks.py` in this folder containing your week 2 implementations\n",
    "2. **Use NumPy only** - All functions must be implemented using [**NumPy**](https://docs.scipy.org/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Overview\n",
    "\n",
    "This assignment provides the **building blocks** for **Neural Networks (NNs)**. You'll learn:\n",
    "\n",
    "- **Fully-connected (Dense) Neural Networks** - Basic network architecture\n",
    "- **Convolutional Neural Networks (CNNs)** - Image processing networks\n",
    "- **Optimization Methods** - Training neural networks\n",
    "- **Image Filtering** - Matrix convolution fundamentals\n",
    "\n",
    "You'll implement these components from scratch to understand how modern deep learning frameworks work.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Note\n",
    "\n",
    "Some concepts may not have been covered in lectures yet. These will be discussed in upcoming sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìë Table of Contents\n",
    "\n",
    "1. [**Fully-Connected Neural Networks**](#1.-Fully-Connected-Neural-Networks)\n",
    "   - [1.1 Dense Layer](#1.1-Dense-layer)\n",
    "   - [1.2 ReLU Nonlinearity](#1.2-ReLU-nonlinearity)\n",
    "   - [1.3 Sigmoid Nonlinearity](#1.3-Sigmoid-nonlinearity)\n",
    "   - [1.4 Sequential Model](#1.4-Sequential-model)\n",
    "   - [1.5 NLL Loss Function](#1.5-NLL-loss-function)\n",
    "   - [1.6 L‚ÇÇ Regularization](#1.6-$L_2$-regularization)\n",
    "   - [1.7 SGD Optimizer](#1.7-SGD-optimizer)\n",
    "2. [**Experiments**](#2.-Experiments)\n",
    "3. [**Convolutions**](#3.-Convolutions)\n",
    "   - [3.1 Matrix Convolution](#3.1-Matrix-convolution)\n",
    "   - [3.2 Basic Kernels](#3.2-Basic-kernels)\n",
    "   - [3.3 Convolutional Layer](#3.3-Convolutional-layer)\n",
    "   - [3.4 Pooling Layer](#3.4-Pooling-layer)\n",
    "   - [3.5 Flatten](#3.5-Flatten)\n",
    "4. [**Image Experiments**](#4.-Image-Experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Current Assignment Grade 70%              |\n",
      "| w1_L2_regression         | completed      |\n",
      "| w1_cal_pseudoinverse     | completed      |\n",
      "| w1_linear_forward        | completed      |\n",
      "| w2_dist_to_training_samples| completed      |\n",
      "| w2_linear_forward        | completed      |\n",
      "| w2_linear_grad_W         | completed      |\n",
      "| w2_linear_grad_b         | completed      |\n",
      "| w2_nearest_neighbors     | completed      |\n",
      "| w2_nll_forward           | completed      |\n",
      "| w2_nll_grad_input        | completed      |\n",
      "| w2_sigmoid_forward       | completed      |\n",
      "| w2_sigmoid_grad_input    | completed      |\n",
      "| w2_tree_split_data_left  | completed      |\n",
      "| w2_tree_split_data_right | completed      |\n",
      "| w2_tree_to_terminal      | completed      |\n",
      "| w2_tree_weighted_entropy | completed      |\n",
      "| w3_box_blur              | not attempted  |\n",
      "| w3_conv_matrix           | not attempted  |\n",
      "| w3_dense_forward         | not attempted  |\n",
      "| w3_flatten_forward       | not attempted  |\n",
      "| w3_l2_regularizer        | not attempted  |\n",
      "| w3_maxpool_forward       | not attempted  |\n",
      "| w3_relu_forward          | not attempted  |\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, absolute_import, division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import blocks\n",
    "\n",
    "import automark as am\n",
    "\n",
    "# fill in you student number as your username\n",
    "username = \"16354842\"\n",
    "\n",
    "# to check your progress, you can run this function\n",
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fully-Connected Neural Networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Neural Network Architecture\n",
    "\n",
    "**Layer composition:** Each layer is a function with parameters (weights):\n",
    "\n",
    "$$\n",
    "h = f(x, w)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $h$ = layer output\n",
    "- $x$ = input vector\n",
    "- $w$ = weight vector\n",
    "\n",
    "**Network as function composition:**\n",
    "\n",
    "Neural networks chain layers together:\n",
    "\n",
    "$$\n",
    "F = f_k \\circ f_{k-1} \\circ \\dots \\circ f_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_1 &= f_1(x, w_1) \\\\\n",
    "h_2 &= f_2(h_1, w_2) \\\\\n",
    "&\\vdots \\\\\n",
    "\\dot{y} &= f_k(h_{k-1}, w_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Parameters:** Different weight vectors $(w_1, w_2, \\ldots, w_k)$ determine the effect of each layer.\n",
    "\n",
    "> **Note:** \"Weights\" are sometimes called \"parameters\", often denoted as $\\theta$.\n",
    "\n",
    "---\n",
    "\n",
    "### üìâ Loss Functions\n",
    "\n",
    "**Purpose:** Measure neural network performance\n",
    "\n",
    "**For classification:** Compare predictions with correct values\n",
    "\n",
    "**Example - Squared Loss:**\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\tfrac{1}{2}\\sum_{n = 1}^N (y_n - \\dot{y}_n)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$ = data point index\n",
    "- $y_n$ = true value\n",
    "- $\\dot{y}_n$ = predicted value\n",
    "\n",
    "**Training objective:** **Minimize the loss function**\n",
    "\n",
    "**Training method:** [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "\n",
    "> **Focus:** This assignment covers the **forward pass**. Backpropagation is implemented for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dense Layer\n",
    "\n",
    "**Also known as:** Fully-connected layer, Multiplicative layer\n",
    "\n",
    "**Function:** Transforms input from one dimension to another\n",
    "\n",
    "$$\n",
    "H = XW + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $H$ = layer output\n",
    "- $X$ = input matrix of size `(n_objects, d_in)`\n",
    "- $W$ = weight matrix of size `(d_in, d_out)`\n",
    "- $b$ = bias vector of size `(d_out,)`\n",
    "\n",
    "**Element-wise formula:**\n",
    "\n",
    "$$\n",
    "H_{nk} = \\sum\\limits_{i=1}^{d_{in}} X_{ni}W_{ik} + b_k\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $n$ = data object index\n",
    "- $k$ = $k^{th}$ output dimension\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Single-layer network classifying 3D points as -1 or 1:\n",
    "- Training set: 75 objects\n",
    "- $X$ shape: `75 √ó 3`\n",
    "- $H$ shape: `75 √ó 1`  \n",
    "- $W$ shape: `3 √ó 1`\n",
    "\n",
    "> **Note:** \"Dense Layer\" is the same as \"Linear\" from week 2, but `n_out` is not restricted to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_dense_forward(x_input, W, b):\n",
    "    \"\"\"Perform the mapping of the input\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the output of a dense layer\n",
    "        np.array of size `(n_objects, n_out)`\n",
    "    \"\"\"\n",
    "    output = x_input.dot(W) + b\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running local tests...\n",
      "w3_dense_forward successfully passed local tests\n",
      "Running remote test...\n",
      "Test was successful. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "am.test_student_function(username, w3_dense_forward, [\"x_input\", \"W\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Current Assignment Grade 74%              |\n",
      "| w1_L2_regression         | completed      |\n",
      "| w1_cal_pseudoinverse     | completed      |\n",
      "| w1_linear_forward        | completed      |\n",
      "| w2_dist_to_training_samples| completed      |\n",
      "| w2_linear_forward        | completed      |\n",
      "| w2_linear_grad_W         | completed      |\n",
      "| w2_linear_grad_b         | completed      |\n",
      "| w2_nearest_neighbors     | completed      |\n",
      "| w2_nll_forward           | completed      |\n",
      "| w2_nll_grad_input        | completed      |\n",
      "| w2_sigmoid_forward       | completed      |\n",
      "| w2_sigmoid_grad_input    | completed      |\n",
      "| w2_tree_split_data_left  | completed      |\n",
      "| w2_tree_split_data_right | completed      |\n",
      "| w2_tree_to_terminal      | completed      |\n",
      "| w2_tree_weighted_entropy | completed      |\n",
      "| w3_box_blur              | not attempted  |\n",
      "| w3_conv_matrix           | not attempted  |\n",
      "| w3_dense_forward         | completed      |\n",
      "| w3_flatten_forward       | not attempted  |\n",
      "| w3_l2_regularizer        | not attempted  |\n",
      "| w3_maxpool_forward       | not attempted  |\n",
      "| w3_relu_forward          | not attempted  |\n"
     ]
    }
   ],
   "source": [
    "am.get_progress(username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîô Backward Pass (Gradient Computation)\n",
    "\n",
    "The backward pass computes gradients using the chain rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_input(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of\n",
    "        the loss with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with\n",
    "            respect to the ouput of the dense layer\n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss with\n",
    "        respect to the input of the layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    grad_input = grad_output.dot(W.T)\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing gradients with respect to weights and bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_W(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of\n",
    "        the loss with respect to W parameter of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with\n",
    "            respect to the ouput of the dense layer\n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss\n",
    "        with respect to W parameter of the layer\n",
    "        np.array of size `(n_in, n_out)`\n",
    "    \"\"\"\n",
    "    grad_W = x_input.T.dot(grad_output)\n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_b(x_input, grad_output, W, b):\n",
    "    \"\"\"Calculate the partial derivative of\n",
    "        the loss with respect to b parameter of the layer\n",
    "    # Arguments\n",
    "        x_input: input of a dense layer - np.array of size `(n_objects, n_in)`\n",
    "        grad_output: partial derivative of the loss functions with\n",
    "            respect to the ouput of the dense layer\n",
    "            np.array of size `(n_objects, n_out)`\n",
    "        W: np.array of size `(n_in, n_out)`\n",
    "        b: np.array of size `(n_out,)`\n",
    "    # Output\n",
    "        the partial derivative of the loss\n",
    "        with respect to b parameter of the layer\n",
    "        np.array of size `(n_out,)`\n",
    "    \"\"\"\n",
    "    grad_b = grad_output.sum(0)\n",
    "    return grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Dense Layer Class\n",
    "\n",
    "Below is the complete implementation based on your functions above (already implemented for you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_phase = True\n",
    "        self.output = 0.0\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "    def get_params(self):\n",
    "        return []\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(Dense, self).__init__()\n",
    "        # Randomly initializing the weights from normal distribution\n",
    "        self.W = np.random.normal(scale=0.01, size=(n_input, n_output))\n",
    "        self.grad_W = np.zeros_like(self.W)\n",
    "        # initializing the bias with zero\n",
    "        self.b = np.zeros(n_output)\n",
    "        self.grad_b = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        self.output = w3_dense_forward(x_input, self.W, self.b)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        # get gradients of weights\n",
    "        self.grad_W = dense_grad_W(x_input, grad_output, self.W, self.b)\n",
    "        self.grad_b = dense_grad_b(x_input, grad_output, self.W, self.b)\n",
    "        # propagate the gradient backwards\n",
    "        return dense_grad_input(x_input, grad_output, self.W, self.b)\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        return [self.grad_W, self.grad_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.60638378 0.15242136]\n",
      " [0.63654187 0.5857447 ]\n",
      " [0.72958596 0.98159449]]\n",
      "[[-0.00482191]\n",
      " [-0.01323934]\n",
      " [-0.02113342]]\n"
     ]
    }
   ],
   "source": [
    "dense_layer = Dense(2, 1)\n",
    "x_input = np.random.random((3, 2))\n",
    "y_output = dense_layer.forward(x_input)\n",
    "print(x_input)\n",
    "print(y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ReLU Nonlinearity\n",
    "\n",
    "**Why nonlinearity?** Combining linear layers is still linear:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H_1 &= XW_1 + b_1\\\\\n",
    "H_2 &= H_1W_2 + b_2\\\\\n",
    "H_2 &= (XW_1 + b_1)W_2 + b_2 \\\\\n",
    "    &= X(W_1W_2) + (b_1W_2 + b_2) \\\\\n",
    "    &= XW^* + b^*\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**With nonlinearity:**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H_1 &= XW_1 + b_1\\\\\n",
    "H_2 &= f(H_1)W_2 + b_2\\\\\n",
    "H_2 &= f(XW_1 + b_1)W_2 + b_2 \\neq XW^* + b^*\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**ReLU (Rectified Linear Unit):**\n",
    "\n",
    "Simple, popular nonlinear activation function (no trainable weights):\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "<img src=\"./src/relu.png\" width=\"500\">\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "\\text{ReLU} \\left(\n",
    "\\begin{bmatrix}\n",
    "1 & -0.5 \\\\\n",
    "0.3 & 0.1 \n",
    "\\end{bmatrix}\n",
    "\\right) = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0.3 & 0.1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Implement the forward pass and backward pass (gradient) for ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_relu_forward(x_input):\n",
    "    \"\"\"relu nonlinearity\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the output of relu layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    output = np.maximum(0, x_input)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0. ]\n",
      " [0.3 0.1]]\n"
     ]
    }
   ],
   "source": [
    "# test forward pass for ReLU, see example above\n",
    "x_input = np.array([[1, -0.5], [0.3, 0.1]])\n",
    "\n",
    "print(w3_relu_forward(x_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running local tests...\n",
      "w3_relu_forward successfully passed local tests\n",
      "Running remote test...\n",
      "Test was successful. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "am.test_student_function(username, w3_relu_forward, [\"x_input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad_input(x_input, grad_output):\n",
    "    \"\"\"relu nonlinearity gradient.\n",
    "        Calculate the partial derivative of the loss\n",
    "        with respect to the input of the layer\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(n_objects, n_in)`\n",
    "            grad_output: np.array of size `(n_objects, n_in)`\n",
    "    # Output\n",
    "        the partial derivative of the loss\n",
    "        with respect to the input of the layer\n",
    "        np.array of size `(n_objects, n_in)`\n",
    "    \"\"\"\n",
    "    grad_input = grad_output * (x_input > 0)\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        self.output = w3_relu_forward(x_input)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        return relu_grad_input(x_input, grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sigmoid Nonlinearity\n",
    "\n",
    "Using the sigmoid implementation from week 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        self.output = blocks.w2_sigmoid_forward(x_input)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        return blocks.w2_sigmoid_grad_input(x_input, grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Sequential Model\n",
    "\n",
    "`SequentialNN` class stores layers and performs basic operations (implemented for you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialNN(object):\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "        self.training_phase = True\n",
    "\n",
    "    def set_training_phase(self, is_training=True):\n",
    "        self.training_phase = is_training\n",
    "        for layer in self.layers:\n",
    "            layer.training_phase = is_training\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        self.output = x_input\n",
    "        for layer in self.layers:\n",
    "            self.output = layer.forward(self.output)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        inputs = [x_input] + [l.output for l in self.layers[:-1]]\n",
    "        for input_, layer_ in zip(inputs[::-1], self.layers[::-1]):\n",
    "            grad_output = layer_.backward(input_, grad_output)\n",
    "\n",
    "    def get_params(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.get_params())\n",
    "        return params\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        grads = []\n",
    "        for layer in self.layers:\n",
    "            grads.extend(layer.get_params_gradients())\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Example Network Architecture\n",
    "\n",
    "Simple neural network taking input of shape `(Any, 10)`:\n",
    "\n",
    "```\n",
    "  INPUT (Any, 10)\n",
    "       |\n",
    "  Dense(10, 4)\n",
    "       |\n",
    "     ReLU\n",
    "       |\n",
    "  Dense(4, 1)\n",
    "       |\n",
    "   Sigmoid\n",
    "       |\n",
    "  OUTPUT (Any, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = SequentialNN(Dense(10, 4), ReLU(), Dense(4, 1), Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'blocks' has no attribute 'w2_sigmoid_forward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m, in \u001b[0;36mSequentialNN.forward\u001b[0;34m(self, x_input)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m x_input\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m, in \u001b[0;36mSigmoid.forward\u001b[0;34m(self, x_input)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_input):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[43mblocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw2_sigmoid_forward\u001b[49m(x_input)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'blocks' has no attribute 'w2_sigmoid_forward'"
     ]
    }
   ],
   "source": [
    "nn.forward(np.ones([2, 10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 NLL Loss Function\n",
    "\n",
    "Loss functions compute both value and gradient (implemented for you):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLL(object):\n",
    "\n",
    "    def forward(self, target_pred, target_true):\n",
    "        self.output = blocks.w2_nll_forward(target_pred, target_true)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, target_pred, target_true):\n",
    "        return blocks.w2_nll_grad_input(target_pred, target_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 L‚ÇÇ Regularization\n",
    "\n",
    "**Purpose:** Penalize model complexity to prevent overfitting\n",
    "\n",
    "**Problem:** Complex models (many parameters, large weights) can overfit training data\n",
    "\n",
    "**Solution:** Add regularization term to loss function\n",
    "\n",
    "**L‚ÇÇ Regularization (Weight Decay):**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^* = \\mathcal{L} + \\frac{\\lambda}{2} \\|w\\|^2_2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\lambda$ = weight decay (hyperparameter controlling regularization strength)\n",
    "- $\\|w\\|^2_2$ = squared [Euclidean norm](https://en.wikipedia.org/wiki/Euclidean_distance)\n",
    "\n",
    "**Expanded form:**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^* = \\mathcal{L} + \\frac{\\lambda}{2} \\sum\\limits_{m=1}^k \\|w_m\\|^2_2\n",
    "$$\n",
    "\n",
    "**Modified weight update:**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_m &\\leftarrow w_m - \\gamma \\frac{\\partial \\mathcal{L}^*}{\\partial w_m}\\\\\n",
    "\\frac{\\partial \\mathcal{L}^*}{\\partial w_m} &= \\frac{\\partial \\mathcal{L}}{\\partial w_m} + \\lambda w_m\\\\\n",
    "w_m &\\leftarrow w_m - \\gamma \\left(\\frac{\\partial \\mathcal{L}}{\\partial w_m} + \\lambda w_m\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Implement L‚ÇÇ regularization:**\n",
    "\n",
    "$$\n",
    "L_2(\\lambda, [w_1, w_2, \\dots, w_k]) = \\frac{\\lambda}{2} \\sum\\limits_{m=1}^k \\|w_m\\|^2_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_l2_regularizer(weight_decay, weights):\n",
    "    \"\"\"Compute the L2 regularization term\n",
    "    # Arguments\n",
    "        weight_decay: float\n",
    "        weights: list of arrays of variable sizes\n",
    "    # Output\n",
    "        L2 regularization term\n",
    "        scalar\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Test Case\n",
    "\n",
    "Expected output: `108.25`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the L2 regularizer\n",
    "weight_decay = 2\n",
    "weights = [np.array([5, 3, 7, 5, 0.5])]\n",
    "print(w3_l2_regularizer(weight_decay, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w3_l2_regularizer, [\"weight_decay\", \"weights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 SGD Optimizer\n",
    "\n",
    "**Stochastic Gradient Descent** - optimization algorithm for training neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent optimizer\n",
    "    https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, lr=0.01, weight_decay=0.0):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def update_params(self):\n",
    "        weights = self.model.get_params()\n",
    "        grads = self.model.get_params_gradients()\n",
    "        for w, dw in zip(weights, grads):\n",
    "            update = self.lr * (dw + self.weight_decay * w)\n",
    "            # it writes the result to the previous variable instead of copying\n",
    "            np.subtract(w, update, out=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiments\n",
    "\n",
    "---\n",
    "\n",
    "Let's test our neural network on the 2 circles dataset from week 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some function from week 2\n",
    "def generate_2_circles(N=100):\n",
    "    phi = np.linspace(0.0, np.pi * 2, 100)\n",
    "    X1 = 1.1 * np.array([np.sin(phi), np.cos(phi)])\n",
    "    X2 = 3.0 * np.array([np.sin(phi), np.cos(phi)])\n",
    "    Y = np.concatenate([np.ones(N), np.zeros(N)]).reshape((-1, 1))\n",
    "    X = np.hstack([X1, X2]).T\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def split(X, Y, train_ratio=0.7):\n",
    "    size = len(X)\n",
    "    train_size = int(size * train_ratio)\n",
    "    indices = np.arange(size)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices = indices[:train_size]\n",
    "    test_indices = indices[train_size:]\n",
    "    return X[train_indices], Y[train_indices], X[test_indices], Y[test_indices]\n",
    "\n",
    "\n",
    "def plot_model_prediction(prediction_func, X, Y, hard=True):\n",
    "    u_min = X[:, 0].min() - 1\n",
    "    u_max = X[:, 0].max() + 1\n",
    "    v_min = X[:, 1].min() - 1\n",
    "    v_max = X[:, 1].max() + 1\n",
    "\n",
    "    U, V = np.meshgrid(np.linspace(u_min, u_max, 100), np.linspace(v_min, v_max, 100))\n",
    "    UV = np.stack([U.ravel(), V.ravel()]).T\n",
    "    c = prediction_func(UV).ravel()\n",
    "    if hard:\n",
    "        c = c > 0.5\n",
    "    plt.scatter(UV[:, 0], UV[:, 1], c=c, edgecolors=\"none\", alpha=0.15)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y.ravel(), edgecolors=\"black\")\n",
    "    plt.xlim(left=u_min, right=u_max)\n",
    "    plt.ylim(bottom=v_min, top=v_max)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = split(*generate_2_circles(), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training the network ##\n",
    "###YOUR CODE FOR DESIGNING THE NETWORK ###\n",
    "model = SequentialNN(\n",
    "    # 2 -> 16 -> 1 With ReLU and Sigmoid where it is required\n",
    ")\n",
    "\n",
    "\n",
    "loss = NLL()\n",
    "weight_decay = 1e-4\n",
    "sgd = SGD(model, lr=0.1, weight_decay=weight_decay)\n",
    "iters = 5000  # Number of times to iterate over all data objects\n",
    "\n",
    "model.set_training_phase(True)\n",
    "\n",
    "for i in range(iters):\n",
    "    # get the predictions\n",
    "    y_pred = model.forward(X_train)\n",
    "\n",
    "    # compute the loss value + L_2 term\n",
    "    loss_value = loss.forward(y_pred, Y_train) + w3_l2_regularizer(\n",
    "        weight_decay, model.get_params()\n",
    "    )\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        # log the current loss value\n",
    "        print(\"Step: {}, \\tLoss = {:.2f}\".format(i, loss_value))\n",
    "\n",
    "    # get the gradient of the loss functions\n",
    "    loss_grad = loss.backward(y_pred, Y_train)\n",
    "\n",
    "    # backprop the gradients\n",
    "    model.backward(X_train, loss_grad)\n",
    "\n",
    "    # perform the updates\n",
    "    sgd.update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_prediction(lambda x: model.forward(x), X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convolutions\n",
    "\n",
    "---\n",
    "\n",
    "### üî≤ 3.1 Matrix Convolution\n",
    "\n",
    "**Locally connected layers** learn local correlations using fewer parameters than dense layers.\n",
    "\n",
    "**Convolutional Layer** is based on **matrix convolution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üñºÔ∏è Visualizing Convolution\n",
    "\n",
    "A picture is worth a thousand words:\n",
    "\n",
    "![Image convolution](./src/conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìê Mathematical Definition\n",
    "\n",
    "**Process:**\n",
    "1. A **filter** (or **kernel**) slides over the source matrix\n",
    "2. Each kernel element multiplies corresponding source element\n",
    "3. Results are summed and written to target matrix\n",
    "\n",
    "**Zero Padding:**\n",
    "- Output is smaller than input (kernel can't overlap borders)\n",
    "- Solution: Add border of zeros to maintain dimensions\n",
    "- Allows kernel to process edge pixels\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "Given source matrix $X$ of size $N \\times M$ and kernel $K$ of size $(2p+1) \\times (2q+1)$:\n",
    "\n",
    "Define $X_{ij} = 0$ for $i > N, i < 1$ and $j > M, j < 1$ (zero padding).\n",
    "\n",
    "$$\n",
    "Y = X \\star K\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y_{ij} = \\sum\\limits_{\\alpha=0}^{2p} \\sum\\limits_{\\beta=0}^{2q}\n",
    "K_{\\alpha \\beta} X_{i + \\alpha - p, j+\\beta - q}\n",
    "$$\n",
    "\n",
    "**Terminology:**\n",
    "- **Machine Learning:** convolution\n",
    "- **Mathematics:** cross-correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úèÔ∏è Exercise: Implement Matrix Convolution\n",
    "\n",
    "Now implement convolution with zero padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_conv_matrix(matrix, kernel):\n",
    "    \"\"\"Perform the convolution of the matrix\n",
    "        with the kernel using zero padding\n",
    "    # Arguments\n",
    "        matrix: input matrix np.array of size `(N, M)`\n",
    "        kernel: kernel of the convolution\n",
    "            np.array of size `(2p + 1, 2q + 1)`\n",
    "    # Output\n",
    "        the result of the convolution\n",
    "        np.array of size `(N, M)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Test Case\n",
    "\n",
    "Test with the following data:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "2 & 3 & 4 \\\\\n",
    "3 & 4 & 5 \\\\\n",
    "\\end{bmatrix} \\quad\n",
    "K = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Expected output:**\n",
    "\n",
    "$$\n",
    "X \\star K = \n",
    "\\begin{bmatrix}\n",
    "7 & 10 & 3 \\\\\n",
    "10 & 14 & 6 \\\\\n",
    "3 & 6 & 8 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "> **Note:** [np.eye](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.eye.html) fills matrix with ones on the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3], [2, 3, 4], [3, 4, 5]])\n",
    "\n",
    "K = np.eye(3)\n",
    "K[-1, -1] = 2\n",
    "print(np.zeros(3))\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your result with the expected output above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w3_conv_matrix(X, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w3_conv_matrix, [\"matrix\", \"kernel\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Basic Kernels\n",
    "\n",
    "**Image processing with convolution** (like Instagram filters): blur, shift, edge detection, etc.\n",
    "\n",
    "**üìñ Recommended read:** [Interactive Image Kernels Article](http://setosa.io/ev/image-kernels/)\n",
    "\n",
    "**Predefined Kernels:**\n",
    "\n",
    "Convolutional layers **learn** kernels through training, but some common kernels exist:\n",
    "\n",
    "**Sharpen Kernel:** \n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 5 & -1 \\\\\n",
    "0 & -1 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Edge Detection Filter:**\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "-1 & 8 & -1 \\\\\n",
    "-1 & -1 & -1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Box Blur (size 3):**\n",
    "$$ \\frac{1}{9}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's experiment with a dog image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_img = plt.imread(\"./images/dog.png\")\n",
    "plt.imshow(rgb_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting to grayscale:**\n",
    "\n",
    "Colored images require 3D tensors (RGB). We convert to grayscale for 2D processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = rgb_img.mean(axis=2)\n",
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Box Blur Implementation\n",
    "\n",
    "**[Box blur](https://en.wikipedia.org/wiki/Box_blur)** - convolution with kernel of size $N \\times N$:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N^2}\n",
    "\\begin{bmatrix}\n",
    "1 & \\dots  & 1\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "1 & \\dots  & 1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Interpretation:** Takes the average of an image region\n",
    "\n",
    "**Arguments:**\n",
    "- `image` - Input matrix `np.array` of size `(N, M)`\n",
    "- `box_size` - Kernel size `int > 0` (kernel is `(box_size, box_size)`)\n",
    "\n",
    "**Output:**  \n",
    "Blurred image `np.array` of size `(N, M)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_box_blur(image, box_size):\n",
    "    \"\"\"Perform the blur of the image\n",
    "    # Arguments\n",
    "        image: input matrix - np.array of size `(N, M)`\n",
    "        box_size: the size of the blur kernel - int > 0\n",
    "            the kernel is of size `(box_size, box_size)`\n",
    "    # Output\n",
    "        the result of the blur\n",
    "            np.array of size `(N, M)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Test Case\n",
    "\n",
    "Running the code should yield:\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 1 \\\\\n",
    "2 & 4 & 2 \\\\\n",
    "1 & 2 & 1 \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.array([[9, 0, 9], [0, 0, 0], [9, 0, 9]])\n",
    "\n",
    "print(w3_box_blur(test_image, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w3_box_blur, [\"image\", \"box_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêï Blurring the Dog\n",
    "\n",
    "Applying box blur to the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_dog = w3_box_blur(img, box_size=3)\n",
    "plt.imshow(blur_dog, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Edge Detection\n",
    "\n",
    "Computing vertical and horizontal gradients:\n",
    "\n",
    "$$\n",
    "K_h = \n",
    "\\begin{bmatrix}\n",
    "-1 & 0  & 1\\\\\n",
    "\\end{bmatrix} \\quad\n",
    "K_v = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "-1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_h = X \\star K_h \\quad X_v = X \\star K_v\n",
    "$$\n",
    "\n",
    "**Gradient amplitude:**\n",
    "\n",
    "$$\n",
    "X_\\text{grad} = \\sqrt{X_h^2 + X_v^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_h = w3_conv_matrix(blur_dog, np.array([[-1, 0, 1]]))\n",
    "dog_v = w3_conv_matrix(blur_dog, np.array([[-1, 0, 1]]).T)\n",
    "dog_grad = np.sqrt(dog_h**2 + dog_v**2)\n",
    "plt.imshow(dog_grad, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces edges of the blurred dog. Other edge detection methods include:\n",
    "- [Canny edge detection](https://en.wikipedia.org/wiki/Canny_edge_detector)\n",
    "- [Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator)\n",
    "- [Prewitt operator](https://en.wikipedia.org/wiki/Prewitt_operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Pattern Detection\n",
    "\n",
    "**Key insight:** Convolving with a kernel produces a **response map**\n",
    "\n",
    "Higher correlation between image patch and kernel ‚Üí Higher response\n",
    "\n",
    "Let's detect a cross pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n",
    "# Create the image\n",
    "image = np.pad(pattern, [(12, 12), (10, 14)], mode=\"constant\", constant_values=0)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.title(\"original image\")\n",
    "plt.show()\n",
    "\n",
    "# Add some noise\n",
    "image = 0.5 * image + 0.5 * np.random.random(image.shape)\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.title(\"noisy image\")\n",
    "plt.show()\n",
    "\n",
    "# Let's find the cross\n",
    "response = w3_conv_matrix(image, pattern)\n",
    "plt.imshow(response, cmap=\"gray\")\n",
    "plt.title(\"local response\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(response == response.max(), cmap=\"gray\")\n",
    "plt.title(\"detected position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brightest pixel shows where the cross is located!\n",
    "\n",
    "**Application:** Find patterns in images (eyes, legs, dogs, cats, etc.)\n",
    "\n",
    "**Next step:** Instead of hand-crafting kernels, we can **learn** them by minimizing loss - that's what **Convolutional Layers** do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Convolutional Layer\n",
    "\n",
    "**Working with images:** 3D tensors of shape $N_{\\text{channels}} \\times H \\times W$\n",
    "\n",
    "- `channels` = color channels (3 for RGB, 1 for grayscale)\n",
    "- $H$ = height\n",
    "- $W$ = width\n",
    "\n",
    "**Batch of images:** 4D tensor of shape $N_{\\text{objects}} \\times N_{\\text{channels}} \\times H \\times W$\n",
    "\n",
    "**Example:** 32 RGB images of size $224 \\times 224$ ‚Üí shape `(32, 3, 224, 224)`\n",
    "\n",
    "---\n",
    "\n",
    "**Convolutional Layer Parameters:**\n",
    "\n",
    "- **Kernels:** Tensor of size `(n_in, n_out, kernel_h, kernel_w)`\n",
    "- **Input:** `(n_objects, n_in, H, W)`\n",
    "- **Output:** `(n_objects, n_out, H, W)`\n",
    "\n",
    "**Process:**\n",
    "1. To get 1st output channel: convolve all input channels with their kernels\n",
    "2. Sum the results\n",
    "3. Write to output channel\n",
    "\n",
    "**Pseudocode:**\n",
    "```python\n",
    "for i in range(n_out):\n",
    "    out_channel = 0.0\n",
    "    for j in range(n_in):\n",
    "        kernel_2d = K[i, j]\n",
    "        input_channel = input_image[j]\n",
    "        out_channel += conv_matrix(input_channel, kernel_2d)\n",
    "    output_image.append(out_channel)\n",
    "```\n",
    "\n",
    "> **Implementation:** Provided for you. Backward pass uses matrix multiplication representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(Layer):\n",
    "    \"\"\"\n",
    "    Convolutional Layer. The implementation is based on\n",
    "        the representation of the convolution as matrix multiplication\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_in, n_out, filter_size):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.W = np.random.normal(size=(n_out, n_in, filter_size, filter_size))\n",
    "        self.b = np.zeros(n_out)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        n_obj, n_in, h, w = x_input.shape\n",
    "        n_out = len(self.W)\n",
    "\n",
    "        self.output = []\n",
    "\n",
    "        for image in x_input:\n",
    "            output_image = []\n",
    "            for i in range(n_out):\n",
    "                out_channel = 0.0\n",
    "                for j in range(n_in):\n",
    "                    out_channel += w3_conv_matrix(image[j], self.W[i, j])\n",
    "                output_image.append(out_channel)\n",
    "            self.output.append(np.stack(output_image, 0))\n",
    "\n",
    "        self.output = np.stack(self.output, 0)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "\n",
    "        N, C, H, W = x_input.shape\n",
    "        F, C, HH, WW = self.W.shape\n",
    "\n",
    "        pad = int((HH - 1) / 2)\n",
    "\n",
    "        self.grad_b = np.sum(grad_output, (0, 2, 3))\n",
    "\n",
    "        # pad input array\n",
    "        x_padded = np.pad(x_input, ((0, 0), (0, 0), (pad, pad), (pad, pad)), \"constant\")\n",
    "        H_padded, W_padded = x_padded.shape[2], x_padded.shape[3]\n",
    "        # naive implementation of im2col\n",
    "        x_cols = None\n",
    "        for i in range(HH, H_padded + 1):\n",
    "            for j in range(WW, W_padded + 1):\n",
    "                for n in range(N):\n",
    "                    field = x_padded[n, :, i - HH : i, j - WW : j].reshape((1, -1))\n",
    "                    if x_cols is None:\n",
    "                        x_cols = field\n",
    "                    else:\n",
    "                        x_cols = np.vstack((x_cols, field))\n",
    "\n",
    "        x_cols = x_cols.T\n",
    "\n",
    "        d_out = grad_output.transpose(1, 2, 3, 0)\n",
    "        dout_cols = d_out.reshape(F, -1)\n",
    "\n",
    "        dw_cols = np.dot(dout_cols, x_cols.T)\n",
    "        self.grad_W = dw_cols.reshape(F, C, HH, WW)\n",
    "\n",
    "        w_cols = self.W.reshape(F, -1)\n",
    "        dx_cols = np.dot(w_cols.T, dout_cols)\n",
    "\n",
    "        dx_padded = np.zeros((N, C, H_padded, W_padded))\n",
    "        idx = 0\n",
    "        for i in range(HH, H_padded + 1):\n",
    "            for j in range(WW, W_padded + 1):\n",
    "                for n in range(N):\n",
    "                    dx_padded[n : n + 1, :, i - HH : i, j - WW : j] += dx_cols[\n",
    "                        :, idx\n",
    "                    ].reshape((1, C, HH, WW))\n",
    "                    idx += 1\n",
    "            dx = dx_padded[:, :, pad:-pad, pad:-pad]\n",
    "        grad_input = dx\n",
    "        return grad_input\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "    def get_params_gradients(self):\n",
    "        return [self.grad_W, self.grad_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Transform 3-channel images to 8-channel images using `3√ó3` kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = ConvLayer(3, 8, filter_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Pooling Layer\n",
    "\n",
    "**Purpose:** Reduce image size (downsampling)\n",
    "\n",
    "**Max Pooling:** Most common pooling operation\n",
    "\n",
    "![Pooling](./src/pool.png)\n",
    "\n",
    "**Effect:**\n",
    "- Reduces spatial dimensions by half (with $2 \\times 2$ windows)\n",
    "- **No effect on depth** (number of channels)\n",
    "\n",
    "**Process:**\n",
    "- Split image into windows\n",
    "- Take maximum value from each window\n",
    "- Use as output\n",
    "\n",
    "![Max Pooling](./src/maxpool.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_maxpool_forward(x_input):\n",
    "    \"\"\"Perform max pooling operation with 2x2 window\n",
    "    # Arguments\n",
    "        x_input: np.array of size (2 * W, 2 * H)\n",
    "    # Output\n",
    "        output: np.array of size (W, H)\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Test Case\n",
    "\n",
    "**Input:**\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 2 & 4 \\\\\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "3 & 2 & 1 & 0 \\\\\n",
    "1 & 2 & 3 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Expected output:**\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "6 & 8 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.array([[1, 1, 2, 4], [5, 6, 7, 8], [3, 2, 1, 0], [1, 2, 3, 4]])\n",
    "\n",
    "print(w3_maxpool_forward(test_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w3_maxpool_forward, [\"x_input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîô Gradient Implementation\n",
    "\n",
    "Backward pass already implemented. Read the code to understand the concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool_grad_input(x_input, grad_output):\n",
    "    \"\"\"Calculate partial derivative of the loss with respect to the input\n",
    "    # Arguments\n",
    "        x_input: np.array of size (2 * W, 2 * H)\n",
    "        grad_output: partial derivative of the loss\n",
    "            with respect to the output\n",
    "            np.array of size (W, H)\n",
    "    # Output\n",
    "        output: partial derivative of the loss\n",
    "            with respect to the input\n",
    "            np.array of size (2 * W, 2 * H)\n",
    "    \"\"\"\n",
    "    height, width = x_input.shape\n",
    "    # create the array of zeros of the required size\n",
    "    grad_input = np.zeros(x_input.shape)\n",
    "\n",
    "    # let's put 1 if the element with this position\n",
    "    # is maximal in the window\n",
    "    for i in range(0, height, 2):\n",
    "        for j in range(0, width, 2):\n",
    "            window = x_input[i : i + 2, j : j + 2]\n",
    "            i_max, j_max = np.unravel_index(np.argmax(window), (2, 2))\n",
    "            grad_input[i + i_max, j + j_max] = 1\n",
    "\n",
    "    # put corresponding gradient instead of 1\n",
    "    grad_input = grad_input.ravel()\n",
    "    grad_input[grad_input == 1] = grad_output.ravel()\n",
    "    grad_input = grad_input.reshape(x_input.shape)\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete MaxPool2x2 Layer implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2x2(Layer):\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        n_obj, n_ch, h, w = x_input.shape\n",
    "        self.output = np.zeros((n_obj, n_ch, h // 2, w // 2))\n",
    "        for i in range(n_obj):\n",
    "            for j in range(n_ch):\n",
    "                self.output[i, j] = w3_maxpool_forward(x_input[i, j])\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        n_obj, n_ch, _, _ = x_input.shape\n",
    "        grad_input = np.zeros_like(x_input)\n",
    "        for i in range(n_obj):\n",
    "            for j in range(n_ch):\n",
    "                grad_input[i, j] = maxpool_grad_input(x_input[i, j], grad_output[i, j])\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Flatten\n",
    "\n",
    "**Purpose:** Bridge convolutional and dense layers\n",
    "\n",
    "**Problem:**\n",
    "- Convolutional layers work with 4D tensors\n",
    "- Dense layers work with 2D matrices (matrices)\n",
    "\n",
    "**Solution:** Flatten layer reshapes tensors\n",
    "\n",
    "**Transformation:**\n",
    "- **Input:** `(n_obj, n_channels, h, w)`\n",
    "- **Output:** `(n_obj, n_channels * h * w)`\n",
    "\n",
    "**Backward pass:** Simply reshape back (no value changes)\n",
    "\n",
    "**Implementation:** Use [np.reshape](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.reshape.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w3_flatten_forward(x_input):\n",
    "    \"\"\"Perform the reshaping of the tensor of size `(K, L, M, N)`\n",
    "        to the tensor of size `(K, L*M*N)`\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(K, L, M, N)`\n",
    "    # Output\n",
    "        output: np.array of size `(K, L*M*N)`\n",
    "    \"\"\"\n",
    "    #################\n",
    "    ### YOUR CODE ###\n",
    "    #################\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß™ Test Case\n",
    "\n",
    "Expected shape: `(100, 768)`\n",
    "\n",
    "> **Note:** We use [np.zeros](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.zeros.html) to test shape transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = np.zeros((100, 3, 16, 16))\n",
    "\n",
    "print(w3_flatten_forward(test_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(username, w3_flatten_forward, [\"x_input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_grad_input(x_input, grad_output):\n",
    "    \"\"\"Calculate partial derivative of the loss with respect to the input\n",
    "    # Arguments\n",
    "        x_input: np.array of size `(K, L, M, N)`\n",
    "        grad_output: partial derivative of the loss\n",
    "            with respect to the output\n",
    "            np.array of size `(K, L*M*N)`\n",
    "    # Output\n",
    "        output: partial derivative of the loss\n",
    "            with respect to the input\n",
    "            np.array of size `(K, L, M, N)`\n",
    "    \"\"\"\n",
    "    grad_input = grad_output.reshape(x_input.shape)\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete Flatten Layer implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer(Layer):\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        self.output = w3_flatten_forward(x_input)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, x_input, grad_output):\n",
    "        output = flatten_grad_input(x_input, grad_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Experiments\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Training with Mini-Batches\n",
    "\n",
    "**Strategy:** Feed small portions of dataset (mini-batches) one-by-one to the network\n",
    "\n",
    "**Benefit:** More efficient training and better generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def iterate_minibatches(x, y, batch_size=16, verbose=True):\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "    indices = np.arange(len(x))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for i, start_idx in enumerate(range(0, len(x) - batch_size + 1, batch_size)):\n",
    "        if verbose:\n",
    "            print(\"\\rBatch: {}/{}\".format(i + 1, len(x) // batch_size), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        excerpt = indices[start_idx : start_idx + batch_size]\n",
    "        yield x[excerpt], y[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Loading MNIST Dataset\n",
    "\n",
    "[Download MNIST dataset](http://yann.lecun.com/exdb/mnist/) first.\n",
    "\n",
    "> **Note:** Unpack downloaded files if you encounter loading errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(load_mnist(dataset=\"training\", path=\".\"))\n",
    "train\n",
    "train_images = np.array([im[1] for im in train])\n",
    "train_targets = np.array([im[0] for im in train])\n",
    "# We will train a 0 vs. 1 classifier\n",
    "x_train = train_images[train_targets < 2][:1000]\n",
    "y_train = train_targets[train_targets < 2][:1000]\n",
    "\n",
    "y_train = y_train\n",
    "y_train = y_train.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¢ About MNIST\n",
    "\n",
    "**MNIST dataset:**\n",
    "- Grayscale images (single channel)\n",
    "- Size: `28√ó28` pixels\n",
    "- RGB values: 0-255\n",
    "- Total: 784 pixels per image\n",
    "\n",
    "**Single image visualization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0].reshape(28, 28), cmap=\"gray_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Preprocessing\n",
    "\n",
    "**Steps:**\n",
    "1. **Normalize** values to [0, 1] for easier optimization\n",
    "2. **Reshape** to add channel dimension `(n_images, 1, 28, 28)`\n",
    "\n",
    "The visual appearance remains unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = x_train.reshape((-1, 1, 28, 28))\n",
    "plt.imshow(x_train[0].reshape(28, 28), cmap=\"gray_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Building a CNN\n",
    "\n",
    "Training a simple convolutional neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn():\n",
    "    nn = SequentialNN(\n",
    "        ConvLayer(1, 2, filter_size=3),  # The output is of size [N_obj 2 28 28]\n",
    "        ReLU(),  # The output is of size [N_obj 2 28 28]\n",
    "        MaxPool2x2(),  # The output is of size [N_obj 2 14 14]\n",
    "        ConvLayer(2, 4, filter_size=3),  # The output is of size [N_obj 4 14 14]\n",
    "        ReLU(),  # The output is of size [N_obj 4 14 14]\n",
    "        MaxPool2x2(),  # The output is of size [N_obj 4 7 7]\n",
    "        FlattenLayer(),  # The output is of size [N_obj 196]\n",
    "        Dense(4 * 7 * 7, 8),\n",
    "        ReLU(),\n",
    "        Dense(8, 1),\n",
    "        Sigmoid(),\n",
    "    )\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = get_cnn()\n",
    "loss = NLL()\n",
    "optimizer = SGD(nn, weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will train for about 5 minutes\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "# We will store the results here\n",
    "history = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "# `num_epochs` represents the number of iterations\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "\n",
    "    # We perform iteration a one-by-one iteration of the mini-batches\n",
    "    for x_batch, y_batch in iterate_minibatches(x_train, y_train, batch_size):\n",
    "        # Predict the target value\n",
    "        y_pred = nn.forward(x_batch)\n",
    "        # Compute the gradient of the loss\n",
    "        loss_grad = loss.backward(y_pred, y_batch)\n",
    "        # Perform backwards pass\n",
    "        nn.backward(x_batch, loss_grad)\n",
    "        # Update the params\n",
    "        optimizer.update_params()\n",
    "\n",
    "        # Save loss and accuracy values\n",
    "        history[\"loss\"].append(loss.forward(y_pred, y_batch))\n",
    "        prediction_is_correct = (y_pred > 0.5) == (y_batch > 0.5)\n",
    "        history[\"accuracy\"].append(np.mean(prediction_is_correct))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the results to get a better insight\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "ax_1 = plt.subplot()\n",
    "ax_1.plot(history[\"loss\"], c=\"g\", lw=2, label=\"train loss\")\n",
    "ax_1.set_ylabel(\"loss\", fontsize=16)\n",
    "ax_1.set_xlabel(\"#batches\", fontsize=16)\n",
    "\n",
    "ax_2 = plt.twinx(ax_1)\n",
    "ax_2.plot(history[\"accuracy\"], lw=3, label=\"train accuracy\")\n",
    "ax_2.set_ylabel(\"accuracy\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí≠ Experiments to Try\n",
    "\n",
    "**Batch size variations:**\n",
    "- `batch_size=1` - What happens?\n",
    "- `batch_size=1000` - What happens?\n",
    "- Does computation speed depend on batch size? Why?\n",
    "\n",
    "**Number of epochs:**\n",
    "- `num_epochs=1` - What happens?\n",
    "- `num_epochs=1000` - What happens?\n",
    "- How does it affect computation time, resources, and accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¨ Visualizing Activations\n",
    "\n",
    "Let's visualize intermediate layer activations to understand what the network learns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_images = x_batch[:2]\n",
    "_ = nn.forward(viz_images)\n",
    "\n",
    "activations = {\n",
    "    \"conv_1\": nn.layers[0].output,\n",
    "    \"relu_1\": nn.layers[1].output,\n",
    "    \"pool_1\": nn.layers[2].output,\n",
    "    \"conv_2\": nn.layers[3].output,\n",
    "    \"relu_2\": nn.layers[4].output,\n",
    "    \"pool_2\": nn.layers[5].output,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(4, 8))\n",
    "\n",
    "ax1.imshow(viz_images[0, 0], cmap=plt.cm.gray_r)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax2.imshow(viz_images[1, 0], cmap=plt.cm.gray_r)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of Conv 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv 1\n",
    "f, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations[\"conv_1\"][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Channel {}\".format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of ReLU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 1\n",
    "f, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations[\"relu_1\"][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Channel {}\".format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of MaxPooling 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling 1\n",
    "f, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations[\"pool_1\"][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Channel {}\".format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of Conv 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv 2\n",
    "f, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations[\"conv_2\"][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Channel {}\".format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of ReLU 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 2\n",
    "f, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations[\"relu_2\"][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Channel {}\".format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations of MaxPooling 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling 2\n",
    "f, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(activations[\"pool_2\"][i, j], cmap=plt.cm.gray_r)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(\"Channel {}\".format(j + 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Understanding Deep Features\n",
    "\n",
    "As we go deeper in the network:\n",
    "- Images become **less locally-correlated** (neighboring pixels less dependent)\n",
    "- Features become more **semantically meaningful**\n",
    "- Each pixel stores more **useful information** about the object\n",
    "- Dense layers at the end analyze these high-level features\n",
    "\n",
    "### üé® More Experiments to Try\n",
    "\n",
    "**Architecture variations:**\n",
    "- Change number of kernels\n",
    "- Vary kernel sizes\n",
    "- Add/remove layers\n",
    "\n",
    "Experiment and observe how performance changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
